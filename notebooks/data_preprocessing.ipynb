{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HROMj0114M5d"
   },
   "source": [
    "# Preprocessing Workflow :\n",
    "\n",
    "- Removing References & Reference Numbers\n",
    "- Removing Page Numbers\n",
    "- Removing Stop words\n",
    "- Removing Punctuation\n",
    "- Removing URLs\n",
    "- Lowercasing\n",
    "- Tokenisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sv_cD5PF6G5O",
    "outputId": "910ae289-a350-4181-d371-78d8f9477156",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# defining lambda function for reading text\n",
    "read = lambda src : open(src,\"r\",errors='ignore').read()\n",
    "\n",
    "# test-reading a paper\n",
    "text = read(\"/Users/tayssirboukrouba/Downloads/dataset/text/0001001v1.txt\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUtpc2MRCjY7"
   },
   "source": [
    "# Testing RegEx Preprocessing Techniques :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x54LuQwoCsJg"
   },
   "source": [
    "### Removing in-text references :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GX3LAoisCx-3",
    "outputId": "a4a01c38-e8fd-4952-f00c-6afb09aeede0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = re.sub(r\"\\[\\d{1,2}\\]|\\(\\d{1,2}\\)\",\"\", text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ui4k4ETDCT2"
   },
   "source": [
    "### Removing Page numberings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptDwbeZODOI2",
    "outputId": "5ee0dd6f-a987-4f65-d4c0-0f45f4244f51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = re.sub(r\"\\s\\d{1,}\\n|^[a-zA-Z]\\n|^[0-9]{1,2}\\n\",\"\", text,flags=re.MULTILINE)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWH69PN1F6m9"
   },
   "source": [
    "### Removing URLs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nr57T68DF9ik",
    "outputId": "f82e62be-ad5a-4a41-f784-a5dbdf4eec30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = re.sub(r'http\\S+|www.\\S+','',text)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqsU5SXpDoi3"
   },
   "source": [
    "### Removing References :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txbNd95UD0wy",
    "outputId": "f20997d8-7dfe-4a7f-a1ad-7c38448638b8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'(?i)(References|Bibliography|Works Cited)(.*)',re.DOTALL)\n",
    "x = re.split(pattern, text)[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_sIkuzIHQA6"
   },
   "source": [
    "### Creating `regex_preprocess()` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuYcnzqKHWZf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def regex_preprocess (text) :\n",
    "  \"\"\"\n",
    "    Preprocesses the input text by applying various regular expression-based transformations.\n",
    "\n",
    "    Steps involved in preprocessing:\n",
    "    1. Removes page numberings and single-lettered lines.\n",
    "    2. Removes in-text references in the form of numbers enclosed in square or round brackets.\n",
    "    3. Removes everything after and including references, bibliography, or works cited sections.\n",
    "    4. Removes all punctuation.\n",
    "    5. Removes all punctuation except for mathematical operation symbols (+, -, *, /) and parentheses/brackets.\n",
    "    6. Removes URLs.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed text.\n",
    "    \"\"\"\n",
    "  # getting rid of page numberings + one-lettered objects\n",
    "  a = re.sub(r\"\\s\\d{1,}\\n|^[a-zA-Z]\\n|^[0-9]{1,2}\\n\",\"\", text,flags=re.MULTILINE)\n",
    "\n",
    "  # getting rid of in-text references\n",
    "  pattern = r\"\\[\\d{1,2}\\]|\\\\d{1,2}\\)\"\n",
    "  b = re.sub(pattern,\"\", a)\n",
    "\n",
    "  # getting rid of references and everything afterwards\n",
    "  pattern = re.compile(r'(References|REFERENCES|Bibliography|Works Cited)\\n(.*)', re.IGNORECASE | re.DOTALL)\n",
    "  c = re.split(pattern, b)[0]\n",
    "\n",
    "\n",
    "  # getting rid of URLs\n",
    "  pattern = r'http\\S+|www.\\S+'\n",
    "  d = re.sub(pattern,'',c)\n",
    "\n",
    "  # getting rid of double space :\n",
    "  #x = re.sub(r\"[\\n\\n]+\",'\\n',x)\n",
    "\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9gktkSsIxli",
    "outputId": "34f1880f-4b51-46f9-909e-e658de62bb98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing it on a paper\n",
    "clean_text = regex_preprocess(text)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8MlMxqXfVcb"
   },
   "source": [
    "# Stop-words & Tokenisation Preprocessing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IEqwywLfZlL",
    "outputId": "47608695-c33b-4ea1-92bf-06c6a4c050b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFokLW0prmUx"
   },
   "source": [
    "## Testing on sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKIfKKhOwEvW",
    "outputId": "e3c32926-ec83-4778-9403-c95ba8f298bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_table = str.maketrans('', '', string.punctuation)\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "# checking first tockenized sentence\n",
    "tokenized_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ej0nrtQ7xcDs",
    "outputId": "7beb10d3-f3a7-40e6-b0f8-9a0605c261fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "joint_sentences = []\n",
    "\n",
    "for tokens in tokenized_sentences:\n",
    "    joint_tokens = ' '.join([token.translate(punctuation_table) for token in tokens if token.lower() not in stop_words])\n",
    "    joint_sentences.append(joint_tokens)\n",
    "\n",
    "print(\"showing random originql sentence : \\n \" , ' '.join(tokenized_sentences[42]))\n",
    "print(\"showing random filtered sentence : \\n \" , joint_sentences[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OyexqlPFyDQu",
    "outputId": "915ad2b8-ef44-4b33-8cef-368dc3b66f6b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_text = '\\n'.join(joint_sentences)\n",
    "print(processed_text[1000:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62h605iSrq6x"
   },
   "source": [
    "## Creating `remove_stop_words()` function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yul06iBorx7g",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(text) :\n",
    "  \"\"\"\n",
    "    Remove stop words from the input text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text containing sentences to be processed.\n",
    "\n",
    "    Returns:\n",
    "    str: Processed text where stop words have been removed from each sentence.\n",
    "         Sentences are separated by newline characters\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenizes the input text into sentences using NLTK's sent_tokenize.\n",
    "    2. Tokenizes each sentence into words using NLTK's word_tokenize and converts them to lowercase.\n",
    "    3. Removes English stop words using NLTK's stopwords.words('english').\n",
    "    4. Joins the remaining tokens back into sentences, preserving sentence boundaries.\n",
    "    5. Returns the processed text where each sentence is on a new line.\n",
    "    \"\"\"\n",
    "\n",
    "  # defining english stop words\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  # getting sentence tokens\n",
    "  sentences = sent_tokenize(text)\n",
    "  # getiing word tokens from sentence tokens\n",
    "  tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "  # deifning reconstructed text list\n",
    "  joint_sentences = []\n",
    "\n",
    "  # looping over word tokens in each sentence\n",
    "  for tokens in tokenized_sentences:\n",
    "      # reconstructing sentence from cleaned tokens\n",
    "      joint_tokens = ' '.join([token for token in tokens if token.lower() not in stop_words])\n",
    "      # appending sentence to the full text list\n",
    "      joint_sentences.append(joint_tokens)\n",
    "\n",
    "  # joining sentences (getting full text)\n",
    "  processed_text = '\\n'.join(joint_sentences)\n",
    "\n",
    "  # returning cleaned text\n",
    "  return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgksajqkzoiG",
    "outputId": "b0646b50-8a0c-40ae-ba98-8bf55f73155b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6ZS651psPZk",
    "outputId": "c24d9896-b1bc-41ff-fe4d-9bafb2c87b22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = read(\"/Users/tayssirboukrouba/Downloads/dataset/text/0001001v1.txt\")\n",
    "processed_text = remove_stop_words(text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXz2tdDbGTkw"
   },
   "source": [
    "## Removing Custom Punctuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x7OIHFwFnKd",
    "outputId": "87fda364-8d9a-43fd-f6a6-9c604597fb03",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting rid of punctuation (except mathematical operations)\n",
    "remove_punct = lambda text : re.sub(r'[.,-:?;\\\"\\']+',\"\",text)\n",
    "text = remove_punct(text)\n",
    "print(text[1000:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKRDCSPBl33J"
   },
   "source": [
    "# Stemming & Lemmatization :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs2TKqfCseHe"
   },
   "source": [
    "## Testing on sample text :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# to do : DONT FORGET TO ADD THEM INTO REQUIREMENTS.txt file\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwn7tzMToPgf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kNeuM-qorLx",
    "outputId": "3caf18e0-c839-4b3c-e93d-f33adbaa7fd7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = read(\"/Users/tayssirboukrouba/Downloads/dataset/text/0001001v1.txt\")\n",
    "old_text = text[1000:2000]\n",
    "doc = nlp(old_text)\n",
    "tokens_list = []\n",
    "for token in doc :\n",
    "  print(token,\"===>\",token.lemma_)\n",
    "  tokens_list.append(token.lemma_)\n",
    "\n",
    "new_text = \" \".join(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx7gk-8LpDZk",
    "outputId": "8660ab0a-a746-4790-c429-700b789c4511"
   },
   "outputs": [],
   "source": [
    "print(\"old text :\\n\", old_text )\n",
    "print(\"-\"*30)\n",
    "print(\"lemmatised text :\\n\", new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxQcPKrxsiU5"
   },
   "source": [
    "## Testing on full text :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wa1OidPra46"
   },
   "outputs": [],
   "source": [
    "old_text = read(\"/Users/tayssirboukrouba/Downloads/dataset/text/0001001v1.txt\")\n",
    "doc = nlp(old_text)\n",
    "tokens_list = []\n",
    "for token in doc :\n",
    "  tokens_list.append(token.lemma_)\n",
    "\n",
    "new_text = \" \".join(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lS9bU0LsoHL",
    "outputId": "ed43d17b-2e83-4edb-ae06-2ea18996f906"
   },
   "outputs": [],
   "source": [
    "print(\"old text :\\n\", old_text[400:600] )\n",
    "print(\"-\"*69)\n",
    "print(\"lemmatised text :\\n\", new_text[500:700] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYpalHPBul6X"
   },
   "source": [
    "## Creating `lemmatize_text()` function :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlSZx6lmuqvz"
   },
   "outputs": [],
   "source": [
    "def lemmatize_text(text) :\n",
    "  \"\"\"\n",
    "    Lemmatizes the input text using SpaCy's en_core_web_sm model.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    - str: The lemmatized text where each word is replaced by its lemma.\n",
    "\n",
    "    Steps:\n",
    "    1. Loads SpaCy's English model 'en_core_web_sm'.\n",
    "    2. Tokenizes the input text into words.\n",
    "    3. Lemmatizes each word to its base form.\n",
    "    4. Joins the lemmatized words back into a single string.\n",
    "    5. Returns the lemmatized text.\n",
    "    \"\"\"\n",
    "\n",
    "  # loading spacy dict\n",
    "  nlp = spacy.load('en_core_web_sm')\n",
    "  # word tokenization\n",
    "  doc = nlp(text)\n",
    "  # defining tokens list\n",
    "  tokens_list = []\n",
    "\n",
    "  # looping over word tokens\n",
    "  for token in doc :\n",
    "    # replacing words by their lemma\n",
    "    tokens_list.append(token.lemma_)\n",
    "\n",
    "  # appending lemmas into text\n",
    "  new_text = \" \".join(tokens_list)\n",
    "\n",
    "  return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E82bAWldx4YU",
    "outputId": "3635461f-e7f5-4b44-c6a4-bb5a7200d31f"
   },
   "outputs": [],
   "source": [
    "help(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq30LONuvnTe"
   },
   "outputs": [],
   "source": [
    "old_text = read(\"/Users/tayssirboukrouba/Downloads/dataset/text/0001001v1.txt\")\n",
    "new_text = lemmatize_text(old_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2F69lFtvwrs",
    "outputId": "072d9dd2-0746-4207-c5b3-ff63c2d9b683"
   },
   "outputs": [],
   "source": [
    "print(\"old text :\\n\", old_text[400:600] )\n",
    "print(\"-\"*69)\n",
    "print(\"lemmatised text :\\n\", new_text[500:700] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6uB380MwRvj"
   },
   "source": [
    "# Combining Preprocessing Pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ir1_WtVNwnmN"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text) :\n",
    "  \"\"\"\n",
    "    Preprocesses the input text by applying several text preprocessing steps.\n",
    "\n",
    "    Args:\n",
    "    - text (str): The input text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    - str: The preprocessed text after applying regex preprocessing, stop-words removal,\n",
    "           custom punctuation removal, and lemmatization.\n",
    "\n",
    "    Steps:\n",
    "    1. Applies regex preprocessing to clean the text (function `regex_preprocess`).\n",
    "    2. Removes stop words from the text (function `remove_stop_words`).\n",
    "    3. Removes custom punctuation from the text (function `remove_punct`).\n",
    "    4. Lemmatizes the text to replace words with their base forms (function `lemmatize_text`).\n",
    "    5. Returns the preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "  # regex preprocessing\n",
    "  a = regex_preprocess(text)\n",
    "\n",
    "  # stop-words preprocessing\n",
    "  b = remove_stop_words(a)\n",
    "\n",
    "  # removing custom punctuation\n",
    "  c = remove_punct(b)\n",
    "\n",
    "  # lemmatizing text\n",
    "  d = lemmatize_text(c)\n",
    "\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WNGnV-Y6ydF4",
    "outputId": "7ef9d5c5-5830-48aa-fa3c-cf1770c06d0a"
   },
   "outputs": [],
   "source": [
    "help(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF1o-5mBzGrQ"
   },
   "outputs": [],
   "source": [
    "text = read(\"/Users/tayssirboukrouba/Downloads/dataset/text/0001001v1.txt\")\n",
    "new = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying The pipeline to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write = lambda filename, text: open(filename, 'w').write(text)\n",
    "\n",
    "filename = '/Users/tayssirboukrouba/Downloads/example.txt'\n",
    "text = 'Hello, my name is taissir'\n",
    "write(filename, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining paths \n",
    "text_path =  \"/Users/tayssirboukrouba/Downloads/dataset/text/\"\n",
    "save_path = \"/Users/tayssirboukrouba/Downloads/dataset/cleaned_text/\"\n",
    "\n",
    "for root, directories, files in os.walk(text_path):\n",
    "  # Access files within the current directory (root)\n",
    "  for filename in tqdm(files, desc=\"Processing text files\"):\n",
    "    # getting file save and read paths\n",
    "    read_filepath = os.path.join(root, filename)\n",
    "    save_filepath = os.path.join(save_path,filename)\n",
    "    if not os.path.exists(save_filepath):\n",
    "        # reading the text file \n",
    "        text = read(read_filepath) \n",
    "        # preprocessing the text \n",
    "        cleaned_text = preprocess_text(text) \n",
    "        write(save_filepath,cleaned_text)\n",
    "        print(f\"{filename} preprocessed and saved successfully in the new directory !\")\n",
    "    else:\n",
    "        print(f\"Skipped '{filename}' (already exists)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "0Ui4k4ETDCT2"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
