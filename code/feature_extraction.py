# -*- coding: utf-8 -*-
"""feature_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y5TYAy5YhtfAC3ss08XEQAHrKGnPGMg3

# Feature Extraction :
"""



import re
import spacy
from datetime import datetime
from spacy.tokenizer import Tokenizer
from spacy.tokens import Doc , Span
import pandas as pd
from spacy import displacy
import nltk
from nltk.tokenize import word_tokenize , sent_tokenize
from nltk.corpus import wordnet as wn
from tqdm import tqdm
import os
import warnings
#nltk.download('punkt')
#nltk.download('wordnet')
warnings.filterwarnings("ignore")

"""## I - Using Weak Labels :"""

# defining lambda function for reading text
read = lambda src : open(src,"r").read()

# test-reading a paper
text = read("data/0001001v1.txt")
print(text[3000:4000])

# getting functions using regex
func_name = r"α-ωA-Za-zΑ-Ω0-9ℰℓℒℳøℂℕℙℚℝℤΓΔΛΞΠΣΦΨΩÅℏ∞∘∂∫∮∯∇αβγ∅"
func_var = r"A-Za-zα-ωΑ-ΩℰℓℒℳøℂℕℙℚℝℤΓΔΛΞΠΣΦΨΩÅℏ∞∘∂∫∮∯∇αβγ∅"

func_pattern = fr"(.|)([{func_name}]{{1,3}}(′|.|)\([{func_var}](,\s*[{func_var}])*\))"

matches = re.findall(func_pattern, text)
for match in matches:
    print(match[1])

"""## II - Using custom Tokenizer :"""

text1 = "This is a text with f(x) and g(x, y) and aǫ(x, k, t) functions."
text = "we deﬁne the ﬂow at the surface by U(x), z = 0..."
# Find math functions
matches = re.findall(func_pattern, text)
# getting only second-group matches (the functions)
functions = [match[1] for match in matches]
func_saver = iter(functions.copy())
# Replace math functions with temporary markers
for func in functions:
  text = text.replace(func, "[FUNC]")

# Tokenize the rest of the text (you can use your preferred tokenizer)
tokens = text.split()
print(f"tokens before : {tokens}")

# Replace temporary markers with original functions
pattern = re.compile(r"(\[FUNC\]).")

#new_tokens = [next(func_saver) if pattern.match(token) else token for token in tokens]
new_tokens = [next(func_saver) if '[FUNC]' in token else token for token in tokens]
print(f"tokens after : {new_tokens}")

def my_tokenizer(text) :

  # defining function pattern
  func_name = r"α-ωA-Za-zΑ-Ω0-9ℰℓℒℳøℂℕℙℚℝℤΓΔΛΞΠΣΦΨΩÅℏ∞∘∂∫∮∯∇αβγ∅"
  func_var = r"A-Za-zα-ωΑ-ΩℰℓℒℳøℂℕℙℚℝℤΓΔΛΞΠΣΦΨΩÅℏ∞∘∂∫∮∯∇αβγ∅"
  func_pattern = fr"(.|)([{func_name}]{{1,3}}(′|.|)\([{func_var}](,\s*[{func_var}])*\))"

  # getting math functions
  matches = re.findall(func_pattern, text)

  # getting only second-group matches (the functions)
  functions = [match[1] for match in matches]
  func_saver = iter(functions.copy())

  # Replace math functions with temporary markers
  for func in functions:
    text = text.replace(func, "[FUNC]")

  # Tokenize the rest of the text
  tokens = text.split()

  # Replace temporary markers with original functions
  new_tokens = [next(func_saver) if '[FUNC]' in token else token for token in tokens]

  return new_tokens

def create_spacy_tokenizer(nlp):
    def custom_tokenizer(text):
        tokens = my_tokenizer(text)
        return Doc(nlp.vocab, words=tokens)
    return custom_tokenizer

nlp = spacy.load("en_core_web_sm")

# adding custom tokenizer
nlp.tokenizer = create_spacy_tokenizer(nlp)

text = "This is a text with f(x) and g(x, y) and aǫ(x, k, t) functions."

# Process the text
doc = nlp(text)

# Iterate over the tokens in the processed doc
for token in doc:
    print(token.text)

"""## II - Using custom NER using Regex weak labels :"""

# testing basic NER

#sample text
text = "The value of epsilon is known as ε = 32 "

# Load the spaCy model
nlp = spacy.load('en_core_web_sm')
doc = nlp(text)

for ent in doc.ents :
  print(ent.text,ent.label_)

# Define symbols to be tagged with custom NER tags
mathematical_symbols = [
    'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω',
    'Α', 'Β', 'Γ', 'Δ', 'Ε', 'Ζ', 'Η', 'Θ', 'Ι', 'Κ', 'Λ', 'Μ', 'Ν', 'Ξ', 'Ο', 'Π', 'Ρ', 'Σ', 'Τ', 'Υ', 'Φ', 'Χ', 'Ψ', 'Ω',
    'ℰ', 'ℓ', 'ℒ', 'ℳ', 'ø', 'ℂ', 'ℕ', 'ℙ', 'ℚ', 'ℝ', 'ℤ','Γ', 'Δ', 'Λ', 'Ξ', 'Π', 'Φ', 'Ψ', 'Ω','Å', 'ℏ', '∞',
    '∂', '∮', '∯', '∇','∅','˜','µ','ǫ','ℋ', 'ℨ', 'ℛ']

math_var = ''.join(mathematical_symbols)
mathematical_operations = ['∫', '∑', '∏', '√', '+', '-', '*', '/', '=', '^', '%','∩', '∪', '⊂', '⊆', '∈', '∉','∘' , "≡","<",">","↔","|"]

mathematical_verbs = ["define","called", "defined as", "used for", "termed as", "represents", "denotes", "stands for", "refers to",
                      "corresponds to", "signifies", "designates", "characterizes as","identified as", "labelled as", "named","outlined as",
                      "identified with", "associated with", "portrays", "symbolizes", "denoted", "denotes","given by", "known as","deﬁne","we choose"]
                      #"is","are"

math_sym = fr"\b([A-Za-zα-ωΑ-Ω|{math_var}]{{1}}(\d)*)\b"

# math structures (functions,sets,lists,vectors)
#\([{(A-Za-zα-ωΑ-Ω|αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩℰℓℒℳøℂℕℙℚℝℤΓΔΛΞΠΦΨΩÅℏ∞∂∮∯∇∅˜µǫℋℨℛ/s*,|\d{1}/s*,)*}]*\)
sets = r"\{[{(A-Za-zα-ωΑ-Ω|{math_var}{{1}}/s*,|\d{{1}}/s*,)*}]*\}"
lists = r"\[[{(A-Za-zα-ωΑ-Ω|{math_var}{{1}}/s*,|\d{{1}}/s*,)*}]*\]"
vectors = r"\([{(A-Za-zα-ωΑ-Ω|{math_var}{{1}}/s*,|\d{{1}}/s*,)*}]*\)"



regex_patterns = [(func_pattern,"MATH_VAR"), (sets,"MATH_VAR"),(lists,"MATH_VAR") , (vectors,"MATH_VAR"),(math_sym,"MATH_VAR")]

# defining custom NER component
@spacy.Language.component("parsing_math_entities")

def parsing_math_entities(doc):
    entities = list(doc.ents)
    for token in doc:
        if token.text in mathematical_symbols:
          entities.append(Span(doc, token.i, token.i + 1, label="MATH_VAR"))
        if token.text in mathematical_operations :
          entities.append(Span(doc,token.i,token.i+1,label="MATH_OPP"))
        elif token.text in mathematical_verbs :
          entities.append(Span(doc,token.i,token.i+1,label="MATH_VRB"))


    # regex patterns
    for pattern , label in regex_patterns:
        for match in re.finditer(pattern, doc.text):
            start, end = match.span()

            # getting rid of white-space start in matches
            if doc.text[start] == " " :
              start_token = doc.char_span(start+1, end)

            else :
              start_token = doc.char_span(start, end)

            if start_token:
                entities.append(Span(doc, start_token.start, start_token.end, label=label))

    # adding more entities
    entities = spacy.util.filter_spans(entities)

    # Updating the document's entities
    doc.ents = entities

    return doc

# Example text
text = "This function denoted as f(x) = k(x,y) and z(x, y,w)"
text2 = "we the ﬂow at the surface by U(x, z = 0..."

nlp = spacy.load("en_core_web_sm")

# adding custom tokenizer
nlp.tokenizer = create_spacy_tokenizer(nlp)

# Adding the custom NER component to the pipeline
nlp.add_pipe("parsing_math_entities",before="ner")

# Process the text with the updated pipeline
doc = nlp(text)

entities = [(ent.text, ent.label_) for ent in doc.ents]
df = pd.DataFrame(entities, columns=['Entity', 'Label'])
df

# defining lambda function for reading text
read = lambda src : open(src,"r").read()

# test-reading a paper
text = read("data/0001001v1.txt")
print(text)

# Process the text with the updated pipeline
doc = nlp(text)

entities = [(ent.text, ent.label_) for ent in doc.ents]
df = pd.DataFrame(entities, columns=['Entity', 'Label'])
df

"""## Visualising Custom NER"""

displacy.render(doc, style="ent", jupyter=True)

"""# Feature Extraction :

- [ ] Clean / get rid of empty entities
- [ ] Get rid of "A"s and "I"s that are not variables
- [ ] Get variable-name using `math_vrb`
- [ ] Get variable-name using `math_var`

# Creating the dataframe :
"""

# defining sample text snippet
sample_text = """We eventually take the limit ε  z/ε and t  x/ε, z  →  →  →  →   ≡  ∼  Since Uz(x, z  0)  ≈  ≈  0, we deﬁne the ﬂow at the surface by  U(x, z = 0)  ≡  U(x) + √εδU(x/ε).
(2.5)  In these rescaled coordinates, U(x) denotes surface ﬂows varying on length scales of O much greater than a typical wavelength, while δU(x/ε) varies over lengths of O(ε) comparable to a typical wavelength.
The amplitude of the slowly varying ﬂow U(x) is O(ε0), while that of the rapidly varying ﬂow δU(x/ε), is assumed to be of O(√ε).
A more detailed discussion of the physical motivation for considering the √ε scaling is deferred to the Results and Discussion.
After rescaling, the boundary conditions (2.4) evaluated at z = 0 become  ∂tη(x, t) +  ∇  ·  U(x) + √εδU(x/ε) (cid:1)  (cid:2)(cid:0)  η(x) (cid:3)  = lim  z→0− ∂zϕ(x, 0)  ∂tϕ(x, t) + U(x)  xϕ(x, t) + √εδU(x/ε)  xϕ(x, t) = ε∆η(x, t)  · ∇  · ∇  (2.6) Although drift that varies slowly along one wavelength can be treated with characteristics and WKB theory, random ﬂows varying on the wavelength scale require a statistical approach.
Without loss of generality, we choose δU to have zero mean and an isotropic x′ two-point correlation function ), where (i, j) = (1, 2) and | denotes an ensemble average over realisations of δU(x).
.
.
.
δUi(x)δUj (x′)
We now deﬁne the spatial Fourier decompositions for the dynamical wave variables  Rij( |  i ≡  −  ε−1η(x, t).
−  ϕ(x,  −  h 6 z 6 ζ, t) =  ϕ(q, t)e−iq·x cosh q(h + z)  cosh qh  Zq  ,  η(x, t) =  Zq  η(q, t)e−iq·x,  the static surface ﬂows  U(x) =  Zq  U(q)e−iq·x,  δU  ε (cid:17)  (cid:16)  =  Zq  δU(q)e−iq·x/ε,  and the correlations  Rij (x) =  Zq  Rij (q)e−iq·x,  (2.7)  (2.8)  (2.9)  where q = (q1, q2) is an in-plane two dimensional wavevector, q  2, and (2π)−2 dq1dq2.
The Fourier integrals for η exclude q = 0 due to the incompress- q ≡ x η(x, t) = 0, while the q = 0 mode for ϕ gives an irrelevant constant ibility constraint shift to the velocity potential.
Note that ϕ in (2.7) manifestly satisﬁes (2.1).
Substituting  |  ≡ |  1 + q2 q2   =
(2.8) into the boundary conditions (2.4), we obtain,  Water wave transport  ∂tη(k, t)   −  Zq  η(k  −  q)U(q)   ·  −  i√ε  Zq  η(k  −  q/ε)δU(q)  ·  k = ϕ(k, t) k tanh εkh  ∂tϕ(k, t)   −  Zq  U(q)  (k  ·  −  q) ϕ(k  q)  −  −  i√ε  Zq  δU(q)  (k  ·  −  q/ε) ϕ(k  q/ε)  −  where the δU(q) are correlated according to  δUi(p)δUj(q)  )δ(p + q).
= Rij ( | |  (εk2 + ε−1)η(k).
=  −  (2.10)  (2.11)  Since the correlation Rij (x) is symmetric in i , Rij( | | | In the case where δU = 0 and U(x)  ) is real.
|  −  U0 is strictly uniform, equations (2.10) can be simpliﬁed by assuming a e−iωt dependence for all dynamical variables.
Uniform drift yields the familiar capillary-gravity wave dispersion relation  ≡  j, and depends only upon the magnitude  ↔  ω(k) =  (k3 + k) tanh kh + U0 ·    Ω(k) + U0 ·  k.  ≡  (2.12)  However, for what follows, we wish to derive transport equations for surface waves (action, energy, intensity) in the presence of a spatially varying drift containing two length scales: U = U(x) + √εδU(x/ε).
"""

# testing split fix :
test_sent = "this is a test sentence where f(x,y) , and g(x,y) are being tested"
pattern = r',\s*(?![^()]*\))'
split_list = re.split(pattern, test_sent)
print(split_list)

def custom_NER_tokenization(text , NER = False) :
  # Load the spaCy model
  nlp = spacy.load("en_core_web_sm")

  # adding custom tokenizer
  nlp.tokenizer = create_spacy_tokenizer(nlp)

  # Adding the custom NER component to the pipeline
  if NER :
    nlp.add_pipe("parsing_math_entities",before="ner")

  # Process the text
  doc = nlp(text)

  return doc

# creating the dataframe
labels = ["MATH_VAR","MATH_STRUCT", "MATH_SYMBOL" , "MATH_OPP" ,"MATH_VRB"]

nlp = spacy.load("en_core_web_sm")

# adding custom tokenizer
nlp.tokenizer = create_spacy_tokenizer(nlp)

# Adding the custom NER component to the pipeline
nlp.add_pipe("parsing_math_entities",before="ner")

sentences = sent_tokenize(sample_text)
# defining variables
sent_of_interest = {"sentences":[],"entities":[] ,"labels" :[]}
pattern = r'[,=]\s*(?![^()]*\))'
for sent in sentences :
  sub_sents = re.split(pattern, sent)
  for sub_sent in sub_sents :
    doc = nlp(sub_sent)
    ent_list = []
    label_list = []
    for ent in doc.ents :
      if ent.label_ in labels :
        ent_list.append(ent.text)
        label_list.append(ent.label_)
    sent_of_interest["sentences"].append(sub_sent)
    sent_of_interest["entities"].append(ent_list)
    sent_of_interest["labels"].append(label_list)

df = pd.DataFrame(sent_of_interest)
df.iloc[0].sentences

df

def create_df(text,pattern,labels) :

  # getting sentence tokens
  sentences = sent_tokenize(text)

  #defining dictionary
  sent_of_interest = {"sentences":[],"entities":[] ,"labels" :[]}
  # looping over sentences
  for sent in sentences :
    # getting sub-sentences (split on "," unless it's between ())
    sub_sents = re.split(pattern, sent)
    # looping over sub-sentences
    for sub_sent in sub_sents :
      # getting tokens out of sub-sentences
      doc = custom_NER_tokenization(sub_sent,True)
      # defining entity/label lists
      ent_list = []
      label_list = []
      # looping over token entities
      for ent in doc.ents :
        # taking only labels in the math-label list
        if ent.label_ in labels :
          # adding labels and entities to their lists
          ent_list.append(ent.text)
          label_list.append(ent.label_)
      # adding the lists to dictionary keys
      sent_of_interest["sentences"].append(sub_sent)
      sent_of_interest["entities"].append(ent_list)
      sent_of_interest["labels"].append(label_list)

  # creating the dataframe
  df = pd.DataFrame(sent_of_interest)

  return df

# defining variables
labels = ["MATH_VAR","MATH_STRUCT", "MATH_SYMBOL" , "MATH_OPP" ,"MATH_VRB"]
pattern = r'[,=]\s*(?![^()]*\))'

df = create_df(sample_text,pattern,labels)
df

"""# Adding POS tags to dataframe :"""

sample_text = "The speed v is known as the velocity"
sam_text = "we deﬁne the ﬂow at the surface by U(x), z = 0..."

# getting tokens from custom tokenizer
doc = custom_NER_tokenization(sam_text)

# Print POS tags
for token in doc:
    print(f"Word: {token.text} | POS: {token.pos_} | Tag: {token.tag_} | desc: {spacy.explain(token.tag_)}")

def POS_tag (text) :
  doc = custom_NER_tokenization(text)
  pos_tags = {"words": [] , "POS" : [] , "tag" : []}

  for token in doc:
    pos_tags["words"].append(token.text)
    pos_tags["POS"].append(token.pos_)
    pos_tags["tag"].append(token.tag_)

  return pos_tags

# testing POS tag function on sample text
text2 = "The flow of surface v is defined to be"
tags = POS_tag(text2)
tags["words"]

# adding the tags to the dataframe
tag_list = []

for sent in df["sentences"] :
  tags = POS_tag(sent)
  tag_list.append(tags["tag"])

df["tags"] = tag_list
df.head()

def add_tags(df) :
  tag_list = []

  for sent in df["sentences"] :
    tags = POS_tag(sent)
    tag_list.append(tags["tag"])

  df["tags"] = tag_list

"""## Dataframe Cleaning :

### Getting rid of false variables :

#### 1- Testing POS taging difference :
"""

text_A_1 = "A football player is named footballer"
text_A_2 = "U(x) denotes surface ﬂows varying on length scales of O much greater than a typical wavelength"

tags_A_1 = POS_tag(text_A_1)
tags_A_2 = POS_tag(text_A_2)

print("First Case A :\n ")
print(tags_A_1["words"])
print(tags_A_1["tag"])
print("-"*50)
print("Second Case A :\n ")
print(tags_A_2["words"])
print(tags_A_2["tag"])

text_I_1 = "I love comedy and i love it so much"
text_I_2 = "the flow of electricity i is equal to "

tags_I_1 = POS_tag(text_I_1)
tags_I_2 = POS_tag(text_I_2)

print("First Case I :\n ")
print(tags_I_1["words"])
print(tags_I_1["tag"])
print("-"*50)
print("Second Case I :\n ")
print(tags_I_2["words"])
print(tags_I_2["tag"])

"""#### 2- Testing getting rid of target words :"""

target_words_A = {'A','a'}
target_words_I = {'i' ,'I'}


target_sent = []
for sent , ent , labels , tags in zip(df["sentences"],df["entities"],df["labels"],df["tags"]) :
  doc = custom_NER_tokenization(sent)
  token_list = [token for token in doc]

  for token in doc :
    token_index = token_list.index(token)
    if (token.text in target_words_A) & (tags[token_index] == "DT") :
      ent_index = ent.index(token.text)
      ent.pop(ent_index)
      labels.pop(ent_index)


    if (token.text in target_words_I) & (token_index == 0) :
      print(f"entitites before : {ent}")
      print(f"labels before : {labels}")
      ent_index = ent.index(token.text)
      ent.pop(ent_index)
      labels.pop(ent_index)
      print(f"entitites after : {ent}")
      print(f"labels after : {labels}")

df

"""## Getting rid of empty entities :"""

condition = df['entities'].apply(lambda ent: len(ent) > 0)
df = df[condition]

df

"""## Checking If sentence is real :

- This means checking if sentence has real words
"""

my_list = ["−" ,"ϕ(x", "−", "h","6","z" ,"6" ,"ζ", "t",")" ,"=", "ϕ(q, t)","e−iq","x" "cos"]

test_list = []
for val in my_list :
  synsets = wn.synsets(val)
  value = len(synsets) > 0
  test_list.append(value)

# counting non-real words
false_count = test_list.count(False)/len(test_list)

if false_count > 0.5 :
    print("Sentence is not real")
else :
  print("Sentence is real")

def is_real_sent(doc):
  test_list = []
  for token in doc :
    synsets = wn.synsets(token.text)
    value = len(synsets) > 0
    test_list.append(value)

  # counting non-real words
  false_count = test_list.count(False)/len(test_list)

  if false_count >= 0.5 :
    return False
  else :
    return True

"""## Compiling Dataframe Cleaning Pipeline :"""

df

tags_to_exclude = []
for sent , ent , labels , tags in zip(df["sentences"],df["entities"],df["labels"],df["tags"]) :
    doc = custom_NER_tokenization(sent)
    if is_real_sent(doc) == False :
      # dropping the row of the false sentence
      df = df[df['sentences'] != sent]
    # if tags less than 4 drop it
    if len(tags)<4 :
      tags_to_exclude.append(tags)

df = df[~df['tags'].isin(tags_to_exclude)]

df

df[df['sentences'] != "The Fourier integrals for η exclude q"]

df['entities'].apply(lambda ent: len(ent) > 0)

condition1 = df['entities'].apply(lambda ent: len(ent) > 0)
condition2 = df['tags'].apply(lambda tags: len(tags) > 4)
df = df[condition1 & condition2]

def clean_df(df) :

  # I - getting rid of empty entities and tags :
  condition = df['entities'].apply(lambda ent: len(ent) > 0)
  condition2 = df['tags'].apply(lambda tags: len(tags) > 4)
  df = df[condition1 & condition2]


  # II - Cleaning dataframe ffrom marginal POS tags
  target_words_A = {'A','a'}
  target_words_I = {'i' ,'I'}
  sent_to_exclude = []

  for sent , ent , labels , tags in zip(df["sentences"],df["entities"],df["labels"],df["tags"]) :
    doc = custom_NER_tokenization(sent)

    if is_real_sent(doc) == False :
      # dropping the row of the false sentence
      df = df[df['sentences'] != sent]

    else :
      token_list = [token for token in doc]
      # looping over tokens
      for token in doc :
        token_index = token_list.index(token)
        # getting rid of marginal "a" and its POS tag
        if (token.text in target_words_A) & (token.text in ent) & (tags[token_index] == "DT") :
          ent_index = ent.index(token.text)
          ent.pop(ent_index)
          labels.pop(ent_index)

        # getting rid of marginal "i" and its POS tag
        if (token.text in target_words_I) & (token.text in ent) & (token_index == 0) :
          ent_index = ent.index(token.text)
          ent.pop(ent_index)
          labels.pop(ent_index)

  return df

"""# Getting `var-name` compound :

### Test Case I :
"""

sent1_1 = "The velocity of an object v"
sent1_2 = "Velocity v of an object"

sent2_1 = "The variables x and y are respectfully the mass and velocity"
sent2_2 = "where a is velocity and b is mass"


tag1_1 = POS_tag(sent1_1)
tag1_2 = POS_tag(sent1_2)


tag2_1 = POS_tag(sent2_1)
tag2_2 = POS_tag(sent2_2)


tag_test_list = [(tag1_1,tag1_2),(tag2_1,tag2_2)]

for (tag1,tag2) in tag_test_list :
  print(f"\ntag case I :\n")
  print(tag1["words"])
  print(tag1["tag"])
  print("*"*20)
  print(f"\ntag case II :\n")
  print(tag2["words"])
  print(tag2["tag"])
  print("-"*100)

sent_list = [sent1_1,sent1_2,sent2_1,sent2_2]
docs = [custom_NER_tokenization(sent,True) for sent in sent_list]
docs

for doc in docs :
  for ent in doc.ents :
    print(ent.text,ent.label_)

"""### Test Case II :"""

test = ["hello","and","tai","bee","and","beb"]
last_index = len(test) - 1 - list(reversed(test)).index("and")
last_index

# doing tests when the variable is in the (begining,middle,end)
sent1 = "v is the velocity of an object"
sent2 = "The variable v is the velocity of an object"
sent3 = "The velocity of an object v"
sent4 = "Velocity of an object v"
sent5 = "Variables a and b are for mass bassis and velocity"
sent6 = "Variables a is mass and b is velocity"


sent_list2 = [sent1,sent2,sent3,sent4,sent5,sent6]
tags2 = [POS_tag(sent) for sent in sent_list2]

for tag in tags2 :
  print(tag["words"])
  print(tag["tag"])
  print("-"*50)

docs2 = [custom_NER_tokenization(sent,True) for sent in sent_list2]
docs2

my_list = []
var_list = []
ent_list = []
for doc in docs2 :
  sub_var_list = []
  sub_ent_list = []
  for ent in doc.ents :
    sub_var_list.append(ent.text)
    sub_ent_list.append(ent.label_)
  var_list.append(sub_var_list)
  ent_list.append(sub_ent_list)


print(var_list)
print(ent_list)

a,b = var_list[-1]
print(a)

ent_list[-1].count('MATH_VAR')

variables = []

for i in range(len(tags2)) :
  #print(ent_list[i].count('MATH_VAR'))
  if ent_list[i].count('MATH_VAR') <2  :
    var = var_list[i][0]
    if var in tags2[i]["words"] :
      var_idx = tags2[i]["words"].index(var)

      # CASE  I : if the variable is at the begining of sentence
      if var_idx == 0 :
        # checking there is a verb after the variable (usually an introductory verb)
        condition1 = tags2[i]["tag"][var_idx+1] == "VBZ"
        # checking if there is a noun after that verb
        condition1_1 = tags2[i]["tag"][var_idx+2] == "NN"
        # or checking if there is "the" after that verb
        condition1_2 =  tags2[i]["words"][var_idx+2] == "the"
        if (condition1) & ((condition1_1) | (condition1_2)) :
          name = " ".join(tags2[i]["words"][var_idx+2:])
          variables.append((var,name))

      # CASE II : if the variable is last
      elif var_idx == len(tags2[i]["words"])-1 :
        # checking if there is a verb before the variable (usually an introductory verb)
        condition2_1 = tags2[i]["tag"][var_idx-1] == "VBZ"
        # checking if there is a noun before the variable
        condition2_2 = tags2[i]["tag"][var_idx-1] == "NN"
        if ((condition2_1) | (condition2_2)) :
          #lowercasing words :
          words = [word.lower() for word in tags2[i]["words"]]

          if ("the" in words) :
            # get the first occurence of the word
            the_idx = words.index("the")
            name = " ".join(tags2[i]["words"][the_idx:var_idx])
            variables.append((var,name))
          else :
            NN_idx = tags2[i]["tag"].index("NN")
            name = " ".join(tags2[i]["words"][NN_idx:var_idx])
            variables.append((var,name))

      # else (if the variable in the middle)
      else :
        condition3_1 = tags2[i]["tag"][var_idx+1] == "VBZ"
        condition3_2 = tags2[i]["words"][var_idx+2] == "the"

        if ((condition3_1)&(condition3_2)) :
          name = " ".join(tags2[i]["words"][var_idx+2:])
          variables.append((var,name))
        else :
          words = [word.lower() for word in tags2[i]["words"]]
          if ("the" in words) :
            the_idx = words.index("the")
            name = " ".join(tags2[i]["words"][the_idx:var_idx])
            variables.append((var,name))
          else :
            NN_idx = tags2[i]["tag"].index("NN")
            name = " ".join(tags2[i]["words"][NN_idx:var_idx])
            variables.append((var,name))

  # dealing with multiple variables
  else :
    word_list = tags2[i]["words"]
    tag_list =  tags2[i]["tag"]
    if word_list.count('and') == 2 :
      # getting the second "and" index
      last_index = len(word_list) - 1 - list(reversed(word_list)).index("and")
      last_verb = len(tag_list) - 1 - list(reversed(tag_list)).index("VBP")

      tag_index = tags2[i]["tag"][last_index]
      name_1 = " ".join(word_list[last_verb+1:last_index])
      name_2 = " ".join(word_list[last_index+1:])
      variables.append((var_list[i][0],name_1))
      variables.append((var_list[i][1],name_2))

    elif word_list.count('and') == 1 :
      and_ind = word_list.index("and")
      var_1_ind = word_list.index(var_list[i][0])
      var_2_ind = word_list.index(var_list[i][1])
      name_1 = " ".join(word_list[var_1_ind+2 : and_ind])
      name_2 = " ".join(word_list[var_2_ind+2 :])
      variables.append((var_list[i][0],name_1))
      variables.append((var_list[i][1],name_2))

variables

"""## Applying variables extraction test cases :"""

def add_to_dict(dictionary,sent,vars,names,position) :
  dictionary["vars"].append(vars)
  dictionary["names"].append(names)
  dictionary["sentence"].append(sent)
  dictionary["variable_position"].append(position)

def var_begin (word_list,tags,var_idx,var,variables,names,positions) :
  # Checking there is a verb after the variable (usually an introductory verb)
  condition1 = tags[var_idx + 1] == "VBZ"
  # Checking if there is a noun after that verb
  condition1_1 = tags[var_idx + 2] == "NN"
  # Or checking if there is "the" after that verb
  condition1_2 = tags[var_idx + 2] == "the"

  if condition1 and (condition1_1 or condition1_2):
    name = " ".join(word_list[var_idx + 2:])
    variables.append(var)
    names.append(name)
    positions.append("start")

  return variables,names,positions

def var_end (word_list,tags,var_idx,var,variables,names,positions) :
  # Checking if there is a verb before the variable (usually an introductory verb)
  condition2_1 = tags[var_idx - 1] == "VBZ"
  # Checking if there is a noun before the variable
  condition2_2 = tags[var_idx - 1] == "NN"
  if condition2_1 or condition2_2:
    # Lowercasing words
    words = [word.lower() for word in word_list]
    if "the" in words:
      # Get the first occurrence of the word "the"
      the_idx = words.index("the")
      name = " ".join(word_list[the_idx:var_idx])
      variables.append(var)
      names.append(name)
      positions.append("end")
    elif "NN" in tags:
      NN_idx = tags.index("NN")
      name = " ".join(word_list[NN_idx:var_idx])
      variables.append(var)
      names.append(name)
      positions.append("end")

  return variables,names,positions

def var_middle(word_list,tags,var_idx,var,variables,names,positions) :
  if var_idx >= len(word_list) - 2 :
    condition3_1 = tags[var_idx + 1] == "VBZ"
    condition3_2 = word_list[var_idx + 2] == "the"
    if condition3_1 and condition3_2:
      name = " ".join(word_list[var_idx + 2:])
      variables.append(var)
      names.append(name)
      positions.append("middle")

    else:
      words = [word.lower() for word in word_list]

      if "the" in words:
        the_idx = words.index("the")
        name = " ".join(word_list[the_idx:var_idx])
        variables.append(var)
        names.append(name)
        positions.append("middle")

      elif "NN" in tags:
        NN_idx = tags.index("NN")
        name = " ".join(word_list[NN_idx:var_idx])
        variables.append(var)
        names.append(name)
        positions.append("middle")

  return variables,names,positions

def single_var(sent,labels,ent,tags,variables,names,word_list,dictionnary,dict_func,positions) :
  # Getting index of label
  ent_idx = labels.index('MATH_VAR')
  # Getting variable from entities
  var = ent[ent_idx]
  # Getting variable position in sentence
  var_idx = word_list.index(var)
  # CASE I:
  if var_idx == 0:
    variables,names,positions = var_begin(word_list,tags,var_idx,var,variables,names,positions)
    dict_func(dictionnary,sent,variables,names,positions)
  # CASE II :
  elif var_idx == len(word_list) - 1 or var_idx == len(word_list) - 2 :
    variables,names,positions = var_end(word_list,tags,var_idx,var,variables,names,positions)
    dict_func(dictionnary,sent,variables,names,positions)
  else:
    variables,names,positions = var_middle(word_list,tags,var_idx,var,variables,names,positions)
    dict_func(dictionnary,sent,variables,names,positions)

def multiple_vars(word_list,tags,var_idx,variables,names,dictionnary,dict_func,positions) :
  if word_list.count('and') == 2 & labels.count("MATH_VAR") ==2:
    # Getting the second "and" index
    last_index = len(word_list) - 1 - list(reversed(word_list)).index("and")
    last_verb = len(tags) - 1 - list(reversed(tags)).index("VBP")
    tag_index = tags[last_index]
    name_1 = " ".join(word_list[last_verb + 1:last_index])
    name_2 = " ".join(word_list[last_index + 1:])
    names.append([name_1, name_2])
    math_ents = []
    for index, label in enumerate(labels):
      if label == "math_var":
        math_ents.append(ent[index])
    variables.append([math_ents[0], math_ents[1]])
    positions.append("multiple")

  elif word_list.count('and') == 1 & labels.count("MATH_VAR") == 2:
    and_ind = word_list.index("and")
    math_ents = []
    for index, label in enumerate(labels):
      if label == "math_var":
        math_ents.append(entities[index])

    var_1_ind = word_list.index(math_ents[0])
    var_2_ind = word_list.index(math_ents[1])
    name_1 = " ".join(word_list[var_1_ind + 2:and_ind])
    name_2 = " ".join(word_list[var_2_ind + 2:])
    names.append([name_1, name_2])
    variables.append([math_ents[0], math_ents[1]])
    positions.append("multiple")

  dict_func(dictionnary,sent,variables,names,positions)

def get_vars_names(dictionnary,dataframe) :
  for sent, ent, labels, tags in zip(dataframe["sentences"], dataframe["entities"], dataframe["labels"], dataframe["tags"]):
    try : 
      doc = custom_NER_tokenization(sent)
      word_list = [token.text for token in doc]
      variables = []
      names = []
      positions = []

      if 'MATH_VAR' in labels and 'and' not in word_list:
        single_var(sent,labels,ent,tags,variables,names,word_list,dictionnary,add_to_dict,positions)

      elif 'MATH_VAR' in labels and 'and' in word_list :
        multiple_vars(word_list,tags,var_idx,variables,names,dictionnary,add_to_dict,positions)
    
    except Exception :
      pass

  return pd.DataFrame(dictionnary)

"""## Creating Feature Extraction Pipeline :"""

labels = ["MATH_VAR","MATH_STRUCT", "MATH_SYMBOL" , "MATH_OPP" ,"MATH_VRB"]
pattern = r'[,=]\s*(?![^()]*\))'
my_dict = {"sentence": [], "vars": [], "names": [] , "variable_position": []}

my_text2 = read("data/0001024v1.txt")
print(my_text2[3000:4000])

# pipeline
# start timer
start_time = datetime.now()
df11 = create_df(my_text2,pattern,labels)
add_tags(df11)
df22 = clean_df(df11)
df33 = get_vars_names(my_dict,df22)
# end timer
end_time = datetime.now()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time}")

df33.tail(40)

df33.iloc[11].names

df33.tail(30)

empty_cond = lambda x: len(x) == 0


df_filtered = df33[~(df33['vars'].apply(empty_cond) & df33['names'].apply(empty_cond))]
df_filtered

my_text3 = read("data/0001001v1.txt")
# pipeline
# start timer
start_time = datetime.now()
dfI = create_df(my_text3,pattern,labels)
add_tags(dfI)
dfII = clean_df(dfI)
dfIII = get_vars_names(my_dict,dfII)
# end timer
end_time = datetime.now()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time}")

dfIII
df_filtered = dfIII[~(dfIII['vars'].apply(empty_cond) & dfIII['names'].apply(empty_cond))]
df_filtered



for i,(name, var_pos) in enumerate(zip(df_filtered["names"], df_filtered["variable_position"])):
    doc = nlp(name[0])
    noun_phrases = [chunk.text for chunk in doc.noun_chunks]
    if noun_phrases :
        if var_pos[0] == "end" :
            df_filtered["names"].iloc[i] = noun_phrases[-1]
        elif var_pos[0] == "start" :
            df_filtered["names"].iloc[i] = noun_phrases[0]

df_filtered

"""# Data Refinement :"""

def refine_df(dataframe) :
    # PHASE I : Cleaning dataframe
    empty_cond = lambda x: len(x) == 0
    # dropping rows with empty vars and names
    dataframe = dataframe[~(dataframe['vars'].apply(empty_cond) & dataframe['names'].apply(empty_cond))]


    # PHASE II : Refining names
    for i,(name, var_pos) in enumerate(zip(dataframe["names"], dataframe["variable_position"])):
        doc = nlp(name[0])
        noun_phrases = [chunk.text for chunk in doc.noun_chunks]
        if noun_phrases :
            if var_pos[0] == "end" :
                dataframe["names"].iloc[i] = noun_phrases[-1]
            elif var_pos[0] == "start" :
                dataframe["names"].iloc[i] = noun_phrases[0]
    return dataframe

labels = ["MATH_VAR","MATH_STRUCT", "MATH_SYMBOL" , "MATH_OPP" ,"MATH_VRB"]
pattern = r'[,=]\s*(?![^()]*\))'
dicttt = {"sentence": [], "vars": [], "names": [] , "variable_position": []}


my_text2 = read("data/0001024v1.txt")
# pipeline
# start timer
start_time = datetime.now()
dfI = create_df(my_text2,pattern,labels)
add_tags(dfI)
dfII = clean_df(dfI)
dfIII = get_vars_names(dicttt,dfII)
dfIII = refine_df(dfIII)
# end timer
end_time = datetime.now()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time}")

dfIII


def feature_extraction(data_path,save_path,dictionnary,labels,pattern,batch_size) :
    # applying the pipeline
    file_counter = 0
    batch_num = 0
    for root, directories, files in os.walk(data_path):
        for filename in tqdm(files, desc="test full pipeline on files"):
            file_counter +=1
            filepath = os.path.join(root, filename)
            # reading the text file
            text = read(filepath)
            dataframe = create_df(text,pattern,labels)
            add_tags(dataframe)
            dataframe = clean_df(dataframe)
            dataframe = get_vars_names(dictionnary,dataframe)
            dataframe = refine_df(dataframe)
        if file_counter % batch_size == 0 :
          batch_num +=1 
          dataframe.to_csv(f"df_{batch_num}.csv",index=False)
    # saving dataframe as csv
    return dataframe

data_path = "data"
save_path = "dataset.csv"
final_dict = {"sentence": [], "vars": [], "names": [] , "variable_position": []}
labels = ["MATH_VAR","MATH_STRUCT", "MATH_SYMBOL" , "MATH_OPP" ,"MATH_VRB"]
pattern = r'[,=]\s*(?![^()]*\))'

# applying full pipeline on all data :
df = feature_extraction(data_path,save_path,final_dict,labels,pattern)
